# Image-Captioning-with-Visual-Attention

## Objective
To build a model that can generate a descriptive caption for an image we provide it.

As you generate a caption, word by word, you can see the model's gaze shifting across the image or in other word, this model can learn where to look.

This is possible because of its Attention mechanism, which allows it to focus on the part of the image most relevant to the word it is going to utter next.

Here are some captions generated on test images not seen during training or validation:
![alt text](https://github.com/sgrvinod/a-PyTorch-Tutorial-to-Image-Captioning/blob/master/img/plane.png)
